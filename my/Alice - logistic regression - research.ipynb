{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Changes\n- plot_cnt = 50\n- timing in curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -l ../input/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!head ../input/train_sessions.csv","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74dae92e7724c0527bcef0c9e0df43c98555ce44","_cell_guid":"fb777394-ad80-455a-b740-3316a97103d5","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c5cf8a9330aa1a9f767a56fde8f48fa842e2fe8","_cell_guid":"0cf36110-0ecb-4c85-9148-6b7b1e0cf55d"},"cell_type":"markdown","source":"We will be solving the intruder detection problem analyzing his behavior on the Internet. It is a complicated and interesting problem combining the data analysis and behavioral psychology.\n\nFor example: Yandex solves the mailbox intruder detection problem based on the user's behavior patterns. In a nutshell, intruder's behaviour pattern might differ from the owner's one: \n- the breaker might not delete emails right after they are read, as the mailbox owner might do\n- the intruder might mark emails and even move the cursor differently\n- etc.\n\nSo the intruder could be detected and thrown out from the mailbox proposing the owner to be authentificated via SMS-code.\nThis pilot project is described in the Habrahabr article.\n\nSimilar things are being developed in Google Analytics and described in scientific researches. You can find more on this topic by searching \"Traversal Pattern Mining\" and \"Sequential Pattern Mining\".\n\nIn this competition we are going to solve a similar problem: our algorithm is supposed to analyze the sequence of websites consequently visited by a particular person and to predict whether this person is Alice or an intruder (someone else). As a metric we will use [ROC AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). We will reveal who Alice is at the end of the course."},{"metadata":{"_uuid":"e6a46d89c9b912f5858baa9930e6280b11750892","_cell_guid":"7e0b3ca3-790c-4bf3-9bec-356d3de9f274"},"cell_type":"markdown","source":"###  Data Downloading and Transformation\nFirst, read the training and test sets. "},{"metadata":{"_uuid":"eb4a1b566bef6a987555baf485809e94e08c31f6","_cell_guid":"a46803c1-86dc-4a19-9007-17e0da493a05","trusted":true},"cell_type":"code","source":"# Read the training and test data sets\ntrain_df = pd.read_csv('../input/train_sessions.csv',\n                       index_col='session_id', parse_dates=['time1'])\ntest_df = pd.read_csv('../input/test_sessions.csv',\n                      index_col='session_id', parse_dates=['time1'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.info())\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_df.info())\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d839d194074efadac75da4dfb5e23e7fa09ac04f","_cell_guid":"dcac415c-7145-4159-8ea6-ed30272c1a43","scrolled":true,"trusted":true},"cell_type":"code","source":"train_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d22359e8d787e79accf47d05c089c52373c8fb4e","_cell_guid":"ae5799dd-3adc-45e8-a203-27459b4ac388"},"cell_type":"markdown","source":"The training data set contains the following features:\n\n- **site1** – id of the first visited website in the session\n- **time1** – visiting time for the first website in the session\n- ...\n- **site10** – id of the tenth visited website in the session\n- **time10** – visiting time for the tenth website in the session\n- **target** – target variable, possesses value of 1 for Alice's sessions, and 0 for the other users' sessions\n    \nUser sessions are chosen in the way they are not longer than half an hour or/and contain more than ten websites. I.e. a session is considered as ended either if a user has visited ten websites or if a session has lasted over thirty minutes.\n\nThere are some empty values in the table, it means that some sessions contain less than ten websites. Replace empty values with 0 and change columns types to integer. Also load the websites dictionary and check how it looks like:"},{"metadata":{"_uuid":"7fb01efc8ae51a6e919382054d0e39ce842cda01","_cell_guid":"0f7b0c81-a791-4842-8b05-30f214507f2f","trusted":true},"cell_type":"code","source":"# Change site1, ..., site10 columns type to integer and fill NA-values with zeros\nsite_feature_names = [x for x in train_df.columns if 'site' in x]\nfor df in (train_df, test_df):\n    df[site_feature_names] = df[site_feature_names].fillna(-1).astype('int')\ntrain_df[site_feature_names].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Add siteMN... features\nsite_feature_names = [x for x in train_df.columns if 'site' in x]\nfor df in (train_df, test_df):\n    for l in range(2,len(site_feature_names)):\n        for i in range(len(site_feature_names)-l+1):\n            print('Progress: %d/%d, %d/%d\\r' % (l,len(site_feature_names)-1,i,len(site_feature_names)-l), end='')\n            df['site' + str(i) + str(l)] = df[site_feature_names[i:i+l]].astype('str').apply(lambda x: '_'.join(x), axis=1)\n\nfeature_names = [x for x in train_df.columns if 'site' in x]\nprint(feature_names[:1000])\nprint(feature_names[-1000:])\ntrain_df[feature_names].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fix timeN columns\ntime_feature_names = [x for x in train_df.columns if 'time' in x]\nfor df in (train_df, test_df):\n    df[time_feature_names] = df[time_feature_names].fillna(0).astype('datetime64')\ntrain_df[time_feature_names].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add dur, durN columns\nfor df in (train_df, test_df):\n    df.drop([x for x in df.columns if 'dur' in x], axis=1, inplace=True)\n\nfor df in (train_df, test_df):\n    df['timex'] = pd.to_datetime(0)\ntime_feature_names = [x for x in train_df.columns if 'time' in x]\nfor df in (train_df, test_df):\n    for i,k in [(x,x+1) for x in range(len(time_feature_names)-1)]:\n        df['dur' + str(i)] = (df[time_feature_names[k]] - df[time_feature_names[i]]).astype('timedelta64[s]').astype('int')\n        df.loc[df[time_feature_names[k]].astype('int') == 0, 'dur' + str(i)] = 0\ndur_feature_names = [x for x in df.columns if 'dur' in x]\nfor df in (train_df, test_df):\n    impute_dur = df[dur_feature_names].median().median()\n    df['dur'] = df[dur_feature_names].sum(axis=1)\n    for i,k in [(x,x+1) for x in range(len(time_feature_names)-1)]:\n        df.loc[df[time_feature_names[k]].astype('int') == 0, 'dur' + str(i)] = impute_dur\nfor df in (train_df, test_df):\n    df.drop('timex', axis=1, inplace=True)\n\nfeature_names = [x for x in train_df.columns if 'dur' in x]\nprint(train_df[feature_names].info())\ntrain_df[feature_names].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add dayN, hourN, minN, monN, domN, weekN columns\ntime_feature_names = [x for x in train_df.columns if 'time' in x]\nfor df in (train_df, test_df):\n    for i in range(len(time_feature_names)):\n        df['cat_day' + str(i)] = df[time_feature_names[i]].dt.dayofweek\n        df['cat_hour' + str(i)] = df[time_feature_names[i]].dt.hour\n        df['cat_min' + str(i)] = df[time_feature_names[i]].dt.minute\n        df['cat_mon' + str(i)] = df[time_feature_names[i]].dt.month\n        df['cat_dom' + str(i)] = df[time_feature_names[i]].dt.day\n        df['cat_week' + str(i)] = df[time_feature_names[i]].dt.week\n        df['cat_day_hour' + str(i)] = df[time_feature_names[i]].dt.dayofweek.astype('str') + '_' + df[time_feature_names[i]].dt.hour.astype('str')\n        cat_feature_names = [x for x in df.columns if 'cat' in x and str(i) in x]\n        df.loc[df[time_feature_names[i]].astype('int') == 0, cat_feature_names] = -1\n        df[cat_feature_names] = df[cat_feature_names].astype('str') # for OHE\n\ncat_feature_names = [x for x in train_df.columns if 'cat' in x]\nprint(train_df[cat_feature_names].info())\ntrain_df[cat_feature_names].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df.info())\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_df.info())\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"site_feature_names = [x for x in train_df.columns if 'site' in x]\ntrain_site_ids = set(train_df[site_feature_names].values.ravel())\ntest_site_ids = set(test_df[site_feature_names].values.ravel())\nlen(train_site_ids), len(test_site_ids), len(test_site_ids - train_site_ids), \\\n    len(test_site_ids & train_site_ids), len(train_site_ids | test_site_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#min(train_site_ids), min(test_site_ids), max(train_site_ids), max(test_site_ids)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using only common features (sites) in train and test data set."},{"metadata":{"trusted":true},"cell_type":"code","source":"common_site_ids = [x for x in train_site_ids & test_site_ids if x != -1]\nlen(common_site_ids)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3c63c91f13663e02c486cb0b545ccadc2dec318","_cell_guid":"2bccda21-944c-4808-83a1-9ca3e1580220"},"cell_type":"markdown","source":"For the very basic model, we will use only the visited websites in the session (but we will not take into account timestamp features). The point behind this data selection is: *Alice has her favorite sites, and the more often you see these sites in the session, the higher probability that this is an Alice's session, and vice versa.*\n\nLet us prepare the data, we will take only features `site1, site2, ... , site10` from the whole dataframe. Keep in mind that the missing values are replaced with zero. Here is how the first rows of the dataframe look like:"},{"metadata":{"_uuid":"8f7b6e88162f02f298cd2edab0aa336d8f377607","_cell_guid":"9f3b378b-a11a-44c9-a7d1-0da152a42aae","trusted":true},"cell_type":"code","source":"y_train = train_df['target'].values\nX_train_feature_names = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ncv_voc = dict([(common_site_ids[i],i) for i in range(len(common_site_ids))])\ndef cv_get_feature_names(cv):\n    rev_voc = dict([(cv.vocabulary_[i],i) for i in cv.vocabulary_])\n    keys = list(rev_voc.keys())\n    keys.sort()\n    vals = []\n    for i in keys:\n        vals += [rev_voc[i]]\n    return vals\ncv = CountVectorizer(analyzer=lambda x: x,vocabulary=cv_voc)\nsite_feature_names = [x for x in train_df.columns if 'site' in x]\nX_train = cv.fit_transform(train_df[site_feature_names].values)\nX_train_feature_names += cv_get_feature_names(cv)\nX_train.shape, len(X_train_feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = [x for x in train_df.columns if 'dur' in x]\nX_train = sparse.hstack([X_train, train_df[feature_names].values])\nX_train_feature_names += feature_names\nX_train.shape, len(X_train_feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ohe = OneHotEncoder(dtype='int64', categories='auto')\ndef ohe_get_feature_names(ohe, names):\n    f_names = []\n    assert(len(names) == len(ohe.categories_))\n    for c_i in range(0,len(ohe.categories_)):\n        f_names += [names[c_i] + '_' + str(f) for f in ohe.categories_[c_i]]\n    return f_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = [x for x in train_df.columns if 'cat' in x]\nX_train = sparse.hstack([X_train, ohe.fit_transform(train_df[feature_names])])\nX_train_feature_names += ohe_get_feature_names(ohe, feature_names)\nX_train.shape, len(X_train_feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = [x for x in X_train_feature_names if '-1' in str(x)]\nprint(feature_names[:1000])\nprint(feature_names[-1000:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove undefined cat features (filled with '-1')\nretain_indices = [i for i in range(len(X_train_feature_names)) if '-1' not in str(X_train_feature_names[i])]\nX_train = X_train.tocsc()[:,retain_indices]\nX_train_feature_names = np.array(X_train_feature_names)[retain_indices]\nX_train.shape, len(X_train_feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(random_state=17, solver='lbfgs', max_iter=500, verbose=3)\nm = Pipeline([('logit', logit)])\nm.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_cnt=50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nsaved_max_iter = m.get_params()['logit__max_iter']\nm.set_params(logit__max_iter=10000)\nsaved_n_jobs = m.get_params()['logit__n_jobs']\nm.set_params(logit__n_jobs=1) # for ConvergenceWarning\n\nprint(m.get_params())\nm.fit(X_train, y_train)\nX_train_feature_imp_coefs = m.named_steps['logit'].coef_.ravel()\nX_train_feature_imp_idxs = np.argsort(X_train_feature_imp_coefs)\nprint(X_train_feature_names[X_train_feature_imp_idxs][:1000])\nprint(X_train_feature_names[X_train_feature_imp_idxs][-1000:])\n\nm.set_params(logit__max_iter=saved_max_iter)\nm.set_params(logit__n_jobs=saved_n_jobs)\nm.get_params()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.tocsc() # for column indexing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_imp(coefs, feature_names, n_top_features=25):\n    positive_coefficients = np.argsort(coefs)[-n_top_features:]\n    negative_coefficients = np.argsort(coefs)[:n_top_features]\n    interesting_coefficients = np.hstack([negative_coefficients, positive_coefficients])\n    colors = [\"red\" if c < 0 else \"blue\" for c in coefs[interesting_coefficients]]\n    plt.figure(figsize=(160, 9))\n    plt.bar(np.array(feature_names)[interesting_coefficients], coefs[interesting_coefficients], color=colors)\n    plt.xticks(rotation=60, ha=\"right\")\n    plt.show()\nplot_imp(X_train_feature_imp_coefs, X_train_feature_names, 250)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Validation curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef plot_vc_features(f_cnt_min, f_cnt_max):\n    xx = range(f_cnt_min, f_cnt_max, int(np.ceil((f_cnt_max-f_cnt_min)/plot_cnt)))\n    vc_test = []\n    for f_cnt in xx:\n        print('Progress: %d%% (%d/%d)\\r' % ((f_cnt-f_cnt_min)*100//(f_cnt_max-f_cnt_min), f_cnt, f_cnt_max), end='')\n        scores = cross_val_score(m, X_train[:,X_train_feature_imp_idxs[-f_cnt:]], y_train,\n                                 cv=5, scoring='roc_auc', n_jobs=-1, verbose=0)\n        vc_test += [scores]\n    plt.plot(xx, np.mean(vc_test, axis=1), c='red', label='test')\n    plt.plot(xx, np.min(vc_test, axis=1), c='blue', label='min')\n    plt.xlabel('Count')\n    plt.ylabel('Score')\n    plt.title('Feature count')\n    plt.legend()\n    plt.show()\n    scores = np.min(vc_test, axis=1)\n    best_idx = np.argmax(scores)\n    return scores[best_idx], xx[best_idx]\nplot_vc_features(10, 60000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nplot_vc_features(1, 1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbest_f = plot_vc_features(1000, 5000)\nbest_f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef plot_vc_C(C_min, C_max):\n    C = np.linspace(C_min, C_max, plot_cnt)\n    grid = GridSearchCV(m, param_grid={'logit__C': C}, return_train_score=True,\n                        cv=5, scoring='roc_auc', n_jobs=-1, verbose=3)\n    grid.fit(X_train[:,X_train_feature_imp_idxs[-best_f[1]:]], y_train)\n    vc_train = np.vstack([grid.cv_results_[x] for x in grid.cv_results_ if 'train_score' in x and 'split' in x]).T\n    vc_test = np.vstack([grid.cv_results_[x] for x in grid.cv_results_ if 'test_score' in x and 'split' in x]).T\n    plt.plot(C, np.mean(vc_train, axis=1), c='green', label='train')\n    plt.plot(C, np.mean(vc_test, axis=1), c='red', label='test')\n    plt.plot(C, np.min(vc_test, axis=1), c='blue', label='min')\n    plt.legend()\n    plt.show()\n    return grid.best_score_, grid.best_params_\nplot_vc_C(1e-5, 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4c85b72086904b7b21e507a2b8029fbdad8575e","_cell_guid":"347e8af0-a5dc-4f55-812a-f0a883849d55"},"cell_type":"markdown","source":"The baseline is **0.91252**\n\nStrong baseline is **0.95965**"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nbest_C = plot_vc_C(1e-5, 10)\nbest_C","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Learning curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndef plot_lc(C):\n    m.set_params(logit__C=C)\n    train_sizes, train_scores, test_scores = learning_curve(\n        m, X_train[:,X_train_feature_imp_idxs[-best_f[1]:]], y_train,\n        train_sizes=np.linspace(0.01,1,plot_cnt),\n        cv=5, scoring='roc_auc', n_jobs=-1, verbose=3,\n        shuffle=True, random_state=17)\n    plt.plot(train_sizes, np.mean(train_scores, axis=1), c='green', label='train')\n    plt.plot(train_sizes, np.mean(test_scores, axis=1), c='red', label='test')\n    plt.plot(train_sizes, np.min(test_scores, axis=1), c='blue', label='min')\n    plt.legend()\n    plt.title('C=' + str(C))\n    plt.show()\nplot_lc(best_C[1]['logit__C'])","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}